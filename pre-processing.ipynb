{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import hypertools as hyp\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# from pylab import plot, show, figure, imshow\n",
    "# %matplotlib notebook\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import RobustScaler, scale\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.constraints import maxnorm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Constant Variables python\n",
    "# kGroup = sys.argv[1]\n",
    "# kInstrument = sys.argv[1]\n",
    "# kSampleRate = 44100\n",
    "# kN = 100\n",
    "# kType = 'mono'\n",
    "# kFeatures = 1812 #Find a way to not hard-code this.     \n",
    "# path = './Music/Data/MedleyDB/Features/%s/%s/' % (sys.argv[2], kType)\n",
    "\n",
    "kInstruments = ['bass3', 'guitar5', 'vocal2', 'keys-all3']\n",
    "kLabels = ['b','g','v','k']\n",
    "kGroup = 'banjo'\n",
    "kInstrument = 'banjo'\n",
    "kSampleRate = 44100\n",
    "kType = 'mono'\n",
    "# path = './Music/Data/MedleyDB/Features/%s/%s/' % (sys.argv[2], kType)\n",
    "path1 = '/homes/mamr3/Stage - 1/Music/Data/MedleyDB/Features/Analysis/' \n",
    "path2 = '/homes/mamr3/Stage - 1/Music/Data/MedleyDB/Features/Regression/' \n",
    "path3 = '/homes/mamr3/Stage - 1/Music/Data/MedleyDB/Features/no_boot_nw_2000/mono/' \n",
    "\n",
    "\n",
    "\n",
    "def dumpPickle(d, name, _path = path1):\n",
    "#  path = './Music/Data/MedleyDB/Features/z_Arrays/%s/' % (kType)\n",
    "\n",
    "  with open(_path + name, 'wb') as output:\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(d, output)\n",
    "  print('%s Saved' % (name))\n",
    "  \n",
    "def loadPickle(name,_path = path1):  \n",
    "#  path = './Music/Data/MedleyDB/Features/z_Arrays/%s/' % (kType)\n",
    "  # load data from pkl file\n",
    "  with open(_path + name, \"rb\") as fp:\n",
    "      loaded_data1 = pickle.load(fp)\n",
    "  \n",
    "  print('%s loaded, %s ' % (name, type(loaded_data1)))\n",
    "  \n",
    "  return loaded_data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_Raw.pkl loaded, <class 'collections.OrderedDict'> \n",
      "X_train.pkl loaded, <class 'collections.OrderedDict'> \n",
      "X_test.pkl loaded, <class 'collections.OrderedDict'> \n",
      "Xs_train.pkl loaded, <class 'collections.OrderedDict'> \n",
      "Xs_test.pkl loaded, <class 'collections.OrderedDict'> \n",
      "Xm_train.pkl loaded, <class 'collections.OrderedDict'> \n",
      "Xm_test.pkl loaded, <class 'collections.OrderedDict'> \n",
      "df_Stem.pkl loaded, <class 'collections.OrderedDict'> \n",
      "y_train.pkl loaded, <class 'collections.OrderedDict'> \n",
      "y_test.pkl loaded, <class 'collections.OrderedDict'> \n",
      "X_All.pkl loaded, <type 'dict'> \n",
      "y_All.pkl loaded, <type 'dict'> \n",
      "X_Raw.pkl loaded, <class 'collections.OrderedDict'> \n",
      "X_Stem.pkl loaded, <class 'collections.OrderedDict'> \n",
      "gNameFeatures.pkl loaded, <class 'collections.OrderedDict'> \n",
      "gPredictionIdx.pkl loaded, <class 'collections.OrderedDict'> \n",
      "gPredictionFeatures.pkl loaded, <class 'collections.OrderedDict'> \n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "#Only if you initialized Variables.\n",
    "\n",
    "# First time I took the files from several folders: \n",
    "\n",
    "# X_All = loadPickle('X.pkl', _path = path2)\n",
    "# Y_All = loadPickle('y.pkl', _path = path2)\n",
    "# XRaw = loadPickle('XRaw.pkl', _path = path3)\n",
    "# XStem = loadPickle('XStem.pkl', _path = path3)\n",
    "# yRaw = loadPickle('yRaw.pkl', _path = path3)\n",
    "# yStem = loadPickle('yStem.pkl', _path = path3)\n",
    "# gNameFeatures = loadPickle('gNameFeatures.pkl', _path = path1)\n",
    "# gPredictionIdx = loadPickle('gPredictionIdx.pkl', _path = path1)\n",
    "# GPredictionFeatures = loadPickle('gPredictionFeatures.pkl', _path = path1)\n",
    "\n",
    "p = '/homes/mamr3/Stage - 1/Music/Data/MedleyDB/Features/Regression/' \n",
    "\n",
    "df_Raw = loadPickle('df_Raw.pkl', _path = p)\n",
    "X_train = loadPickle('X_train.pkl', _path = p)\n",
    "X_test = loadPickle('X_test.pkl', _path = p)\n",
    "Xs_train = loadPickle('Xs_train.pkl', _path = p)\n",
    "Xs_test = loadPickle('Xs_test.pkl', _path = p)\n",
    "Xm_train = loadPickle('Xm_train.pkl', _path = p)\n",
    "Xm_test = loadPickle('Xm_test.pkl', _path = p)\n",
    "df_Stem = loadPickle('df_Stem.pkl', _path = p)\n",
    "y_train = loadPickle('y_train.pkl', _path = p)\n",
    "y_test = loadPickle('y_test.pkl', _path = p)\n",
    "\n",
    "X_All = loadPickle('X_All.pkl', _path = p)\n",
    "Y_All = loadPickle('y_All.pkl', _path = p)\n",
    "X_Raw = loadPickle('X_Raw.pkl', _path = p)\n",
    "X_Stem = loadPickle('X_Stem.pkl', _path = p)\n",
    "\n",
    "gNameFeatures = loadPickle('gNameFeatures.pkl', _path = p)\n",
    "gPredictionIdx = loadPickle('gPredictionIdx.pkl', _path = p)\n",
    "GPredictionFeatures = loadPickle('gPredictionFeatures.pkl', _path = p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gets dataframes of Predicted Feature values.\n",
    "\n",
    "def getDFfromPickle(dataX):\n",
    "    XPrediction = OrderedDict()\n",
    "    gPredictionFeatures = OrderedDict()\n",
    "    for k in kInstruments:\n",
    "        #print '\\n %s \\n' % k.upper()        \n",
    "        _A = dataX[k[0]][:]\n",
    "        Xpreselected = [] \n",
    "        for i in gPredictionIdx[k]:\n",
    "\n",
    "            Xpreselected.append(_A[:,i])\n",
    "\n",
    "        XPrediction[k] = np.asarray(Xpreselected).T \n",
    "\n",
    "        x = []\n",
    "        for f in GPredictionFeatures[k]:\n",
    "            x.append('.'.join(f.split('.')[1::]))\n",
    "        gPredictionFeatures[k] = x\n",
    "\n",
    "    dfBass = pd.DataFrame(XPrediction[kInstruments[0]], columns = gPredictionFeatures[kInstruments[0]])\n",
    "    dfGuitar = pd.DataFrame(XPrediction[kInstruments[1]], columns = gPredictionFeatures[kInstruments[1]])\n",
    "    dfVocal = pd.DataFrame(XPrediction[kInstruments[2]], columns = gPredictionFeatures[kInstruments[2]])\n",
    "    dfKeys = pd.DataFrame(XPrediction[kInstruments[3]], columns = gPredictionFeatures[kInstruments[3]])\n",
    "    \n",
    "    df = OrderedDict()\n",
    "    df[kLabels[0]] = dfBass\n",
    "    df[kLabels[1]] = dfGuitar\n",
    "    df[kLabels[2]] = dfVocal\n",
    "    df[kLabels[3]] = dfKeys\n",
    "    \n",
    "    return df\n",
    "\n",
    "def getNPfromDF(df_R, df_S, trainSize = 0.9):\n",
    "    \n",
    "    A = OrderedDict()\n",
    "    A_train = OrderedDict()\n",
    "    A_test = OrderedDict() \n",
    "    As = OrderedDict()\n",
    "    As_train = OrderedDict()\n",
    "    As_test = OrderedDict() \n",
    "    Am = OrderedDict()\n",
    "    Am_train = OrderedDict()\n",
    "    Am_test = OrderedDict() \n",
    "    b = OrderedDict()\n",
    "    b_train = OrderedDict()\n",
    "    b_test = OrderedDict()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in kLabels:\n",
    "        \n",
    "        df = df_Raw[k]\n",
    "        df = df[np.abs(df-df.mean())<=(3*df.std())]\n",
    "        idx = []\n",
    "        for i in df.columns:\n",
    "            a = pd.notnull(df[i])\n",
    "            try:\n",
    "                idx.append(np.where( a == False )[0][0])\n",
    "                idx.append(np.where( a == False )[0][1])\n",
    "            except:\n",
    "                pass\n",
    "        idx = list(set(idx))\n",
    "        df.drop(df.index[idx], inplace=True)\n",
    "\n",
    "        norm_df=(df-df.mean())/df.std()\n",
    "        minmax_df=(df-df.min())/(df.max()-df.min())\n",
    "        \n",
    "        A[k] = pd.DataFrame.as_matrix(df)\n",
    "        As[k] = pd.DataFrame.as_matrix(norm_df)\n",
    "        Am[k] = pd.DataFrame.as_matrix(minmax_df)\n",
    "        df2 = df_S[k]\n",
    "        df2.drop(df2.index[idx], inplace=True)\n",
    "        b[k] = pd.DataFrame.as_matrix(df2)\n",
    "        A_train[k], A_test[k], As_train[k], As_test[k], Am_train[k], Am_test[k],b_train[k], b_test[k] = train_test_split(A[k],\n",
    "                                                                                                                         As[k],\n",
    "                                                                                                                         Am[k],\n",
    "                                                                                                                         b[k],\n",
    "                                                                                                                         train_size=trainSize,\n",
    "                                                                                                                         random_state=42)\n",
    "    return A_train, A_test, As_train, As_test, Am_train, Am_test, b_train, b_test\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_Raw = OrderedDict()\n",
    "# X_Stem = OrderedDict()\n",
    "# X_Raw['b'] = XRaw['electric bass']\n",
    "# X_Raw['g'] = np.vstack((XRaw['acoustic guitar'], \n",
    "#                         XRaw['clean electric guitar'], \n",
    "#                         XRaw['distorted electric guitar'], \n",
    "#                         XRaw['banjo']))\n",
    "# X_Raw['v'] = np.vstack((XRaw['male singer'], \n",
    "#                         XRaw['female singer'], \n",
    "#                         XRaw['male rapper']))\n",
    "# X_Raw['k'] = np.vstack((XRaw['piano'], \n",
    "#                         XRaw['synthesizer'], \n",
    "#                         XRaw['tack piano'], \n",
    "#                         XRaw['electric piano']))\n",
    "# X_Stem['b'] = XStem['electric bass']\n",
    "# X_Stem['g'] = np.vstack((XStem['acoustic guitar'], \n",
    "#                         XStem['clean electric guitar'], \n",
    "#                         XStem['distorted electric guitar'], \n",
    "#                         XStem['banjo']))\n",
    "# X_Stem['v'] = np.vstack((XStem['male singer'], \n",
    "#                         XStem['female singer'], \n",
    "#                         XStem['male rapper']))\n",
    "# X_Stem['k'] = np.vstack((XStem['piano'], \n",
    "#                         XStem['synthesizer'], \n",
    "#                         XStem['tack piano'], \n",
    "#                         XStem['electric piano']))\n",
    "\n",
    "\n",
    "\n",
    "# df_Raw = getDFfromPickle(X_Raw)\n",
    "# df_Stem = getDFfromPickle(X_Stem)\n",
    "\n",
    "# X_train, X_test, Xs_train, Xs_test, Xm_train, Xm_test, y_train, y_test = getNPfromDF(df_Raw, df_Stem, trainSize = 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = '/homes/mamr3/Stage - 1/Music/Data/MedleyDB/Features/Regression/' \n",
    "\n",
    "# dumpPickle(df_Raw, 'df_Raw.pkl', _path = p)\n",
    "# dumpPickle(df_Stem, 'df_Stem.pkl', _path = p)\n",
    "# dumpPickle(X_train, 'X_train.pkl', _path = p)\n",
    "# dumpPickle(X_test, 'X_test.pkl', _path = p)\n",
    "# dumpPickle(Xs_train, 'Xs_train.pkl', _path = p)\n",
    "# dumpPickle(Xs_test, 'Xs_test.pkl', _path = p)\n",
    "# dumpPickle(Xm_train, 'Xm_train.pkl', _path = p)\n",
    "# dumpPickle(Xm_test, 'Xm_test.pkl', _path = p)\n",
    "# dumpPickle(df_Stem, 'df_Stem.pkl', _path = p)\n",
    "# dumpPickle(y_train, 'y_train.pkl', _path = p)\n",
    "# dumpPickle(y_test, 'y_test.pkl', _path = p)\n",
    "\n",
    "# dumpPickle(X_All, 'X_All.pkl', _path = p)\n",
    "# dumpPickle(Y_All, 'y_All.pkl', _path = p)\n",
    "# dumpPickle(X_Raw, 'X_Raw.pkl', _path = p)\n",
    "# dumpPickle(X_Stem, 'X_Stem.pkl', _path = p)\n",
    "\n",
    "# dumpPickle(gNameFeatures, 'gNameFeatures.pkl', _path = p)\n",
    "# dumpPickle(gPredictionIdx, 'gPredictionIdx.pkl', _path = p)\n",
    "# dumpPickle(GPredictionFeatures, 'gPredictionFeatures.pkl', _path = p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def model_1(neurons=32, init_mode='zero', act_function='selu', dropout_rate=0.0, weight_constraint=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons*2, input_dim=4, \n",
    "                    kernel_initializer=init_mode, \n",
    "                    activation=act_function, \n",
    "                    kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons, kernel_initializer=init_mode, activation=act_function,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4, kernel_initializer=init_mode))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model\n",
    "# define base model\n",
    "\n",
    "def model_2(neurons=32,init_mode='zero', act_function='selu', dropout_rate=0.0, weight_constraint=0):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons*2, input_dim=4, \n",
    "                    kernel_initializer=init_mode, \n",
    "                    activation=act_function, \n",
    "                    kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neurons, kernel_initializer=init_mode, activation=act_function,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(6, kernel_initializer=init_mode))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "# estimator_b = KerasRegressor(build_fn=model_1, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_bRS = KerasRegressor(build_fn=model_1, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_bS = KerasRegressor(build_fn=model_1, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_g = KerasRegressor(build_fn=model_2, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_gRS = KerasRegressor(build_fn=model_2, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_gS = KerasRegressor(build_fn=model_2, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_k = KerasRegressor(build_fn=model_1, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_kRS = KerasRegressor(build_fn=model_1, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_kS = KerasRegressor(build_fn=model_1, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_v = KerasRegressor(build_fn=model_1, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_vRS = KerasRegressor(build_fn=model_1, nb_epoch=100, batch_size=5, verbose=0)\n",
    "# estimator_vS = KerasRegressor(build_fn=model_1, nb_epoch=100, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 3.96 (0.07) MAE\n",
      "Results - S: 3.96 (0.07) MAE\n",
      "Results - M: 3.96 (0.07) MAE\n"
     ]
    }
   ],
   "source": [
    "# label = 'b'\n",
    "# A_train = (X_train[label])\n",
    "# b_train = (y_train[label])\n",
    "# b_test = (y_test[label])\n",
    "# A_test = (X_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_b, A_train, b_train, cv=kfold)\n",
    "# print(\"Results: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n",
    "\n",
    "\n",
    "# A_train = (Xs_train[label])\n",
    "# A_test = (Xs_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_bRS, A_train, b_train, cv=kfold)\n",
    "# print(\"Results - S: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n",
    "\n",
    "\n",
    "# A_train = (Xm_train[label])\n",
    "# A_test = (Xm_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_bS, A_train, b_train, cv=kfold)\n",
    "# print(\"Results - M: %.2f (%.2f) MAE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 1.33 (0.05) MAE\n",
      "Results - RS: 1.33 (0.05) MAE\n",
      "Results - S: 1.33 (0.05) MAE\n"
     ]
    }
   ],
   "source": [
    "# label = 'g'\n",
    "# A_train = (X_train[label])\n",
    "# b_train = (y_train[label])\n",
    "# b_test = (y_test[label])\n",
    "# A_test = (X_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_g, A_train, b_train, cv=kfold)\n",
    "# print(\"Results: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n",
    "\n",
    "\n",
    "# A_train = (Xs_train[label])\n",
    "# A_test = (Xs_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_gRS, A_train, b_train, cv=kfold)\n",
    "# print(\"Results - RS: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n",
    "\n",
    "\n",
    "# A_train = (Xm_train[label])\n",
    "# A_test = (Xm_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_gS, A_train, b_train, cv=kfold)\n",
    "# print(\"Results - S: %.2f (%.2f) MAE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 7.94 (0.42) MAE\n",
      "Results - RS: 7.94 (0.42) MAE\n",
      "Results - S: 7.94 (0.42) MAE\n"
     ]
    }
   ],
   "source": [
    "# label = 'k'\n",
    "# A_train = (X_train[label])\n",
    "# b_train = (y_train[label])\n",
    "# b_test = (y_test[label])\n",
    "# A_test = (X_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_k, A_train, b_train, cv=kfold)\n",
    "# print(\"Results: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n",
    "\n",
    "\n",
    "# A_train = (Xs_train[label])\n",
    "# A_test = (Xs_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_kRS, A_train, b_train, cv=kfold)\n",
    "# print(\"Results - RS: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n",
    "\n",
    "\n",
    "# A_train = (Xm_train[label])\n",
    "# A_test = (Xm_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_kS, A_train, b_train, cv=kfold)\n",
    "# print(\"Results - S: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 2.45 (0.46) MAE\n",
      "Results - RS: 2.45 (0.46) MAE\n",
      "Results - S: 2.45 (0.46) MAE\n"
     ]
    }
   ],
   "source": [
    "# label = 'v'\n",
    "# A_train = (X_train[label])\n",
    "# b_train = (y_train[label])\n",
    "# b_test = (y_test[label])\n",
    "# A_test = (X_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_v, A_train, b_train, cv=kfold)\n",
    "# print(\"Results: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n",
    "\n",
    "\n",
    "# A_train = (Xs_train[label])\n",
    "# A_test = (Xs_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_vRS, A_train, b_train, cv=kfold)\n",
    "# print(\"Results - RS: %.2f (%.2f) MAE\" % (results.mean(), results.std()))\n",
    "\n",
    "\n",
    "# A_train = (Xm_train[label])\n",
    "# A_test = (Xm_test[label])\n",
    "\n",
    "# kfold = KFold(n_splits=5, random_state=seed)\n",
    "# results = cross_val_score(estimator_vS, A_train, b_train, cv=kfold)\n",
    "# print(\"Results - S: %.2f (%.2f) MAE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = sys.argv[1]\n",
    "A_train = (X_train[label])\n",
    "b_train = (y_train[label])\n",
    "b_test = (y_test[label])\n",
    "A_test = (X_test[label])\n",
    "\n",
    "\n",
    "batch_size = [10, 20, 40, 50, 60, 70]\n",
    "epochs = [50, 100, 150, 200]\n",
    "init_mode = ['zero','uniform','lecun_uniform', 'normal']\n",
    "activation = ['selu','relu','elu','tanh','sigmoid']\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
    "neurons = [8, 16, 32, 64, 128]\n",
    "\n",
    "\n",
    "est = KerasRegressor(build_fn=model_1, verbose=0)\n",
    "\n",
    "\n",
    "param_grid = dict(neurons=neurons,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  dropout_rate=dropout_rate,\n",
    "                  weight_constraint=weight_constraint,\n",
    "                  init_mode=init_mode,\n",
    "                  act_function=activation)\n",
    "\n",
    "grid = GridSearchCV(estimator=est, param_grid=param_grid, n_jobs=4)\n",
    "grid_result = grid.fit(A_train, b_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n Bass - Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# def R2(y_true, y_pred):\n",
    "#     return r2_score(y_true, y_pred)\n",
    "\n",
    "# label = 'k'\n",
    "# x_train = (Xs_train[label])\n",
    "# Y_train = (y_train[label][:,0])\n",
    "# Y_test = (y_test[label][:,0])\n",
    "# x_test = (Xs_test[label])\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(64, input_dim=4, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(4, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# model.compile(loss='mean_absolute_error',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(x_train, Y_train, epochs=160, verbose=1, validation_split=0.05, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(x_test)\n",
    "# r2_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# max_depth = 30\n",
    "# regr_multirf = MultiOutputRegressor(RandomForestRegressor(max_depth=max_depth,\n",
    "#                                                           random_state=0))\n",
    "# regr_multirf.fit(x_train, Y_train)\n",
    "\n",
    "# regr_rf = RandomForestRegressor(n_estimators=2000, max_depth=None, random_state=None)\n",
    "# regr_rf.fit(x_train, Y_train)\n",
    "\n",
    "# # Predict on new data\n",
    "# y_multirf = regr_multirf.predict(x_test)\n",
    "# y_rf = regr_rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# r2_score(Y_test, y_multirf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# r2_score(Y_test, y_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bass\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "hyp.plot(dfBass,\n",
    "         ['o','^'],\n",
    "         group = y[kInstruments[0]],\n",
    "         normalize = 'within',\n",
    "         legend = ['raw','stem'],\n",
    "         ndims = 2,\n",
    "         #explore = True,\n",
    "         #animate = True, duration = 30, tail_duration = 10\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Guitar\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "hyp.plot(dfGuitar,\n",
    "         ['o','^'],\n",
    "         group = y[kInstruments[1]],\n",
    "         normalize = 'within',\n",
    "         legend = ['raw','stem'],\n",
    "         ndims = 2,\n",
    "         #explore = True,\n",
    "         #animate = True, duration = 30, tail_duration = 10\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vocal\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "hyp.plot(dfVocal,\n",
    "         ['o','^'],\n",
    "         group = y[kInstruments[2]],\n",
    "         normalize = 'within',\n",
    "         legend = ['raw','stem'],\n",
    "         ndims = 2,\n",
    "         #explore = True,\n",
    "         #animate = True, duration = 30, tail_duration = 10\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keys\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "hyp.plot(dfKeys,\n",
    "         ['o','^'],\n",
    "         group = y[kInstruments[3]],\n",
    "         normalize = 'within',\n",
    "         legend = ['raw','stem'],\n",
    "         ndims = 2,\n",
    "         #explore = True,\n",
    "         #animate = True, duration = 30, tail_duration = 10\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quantitative analysis:   \n",
    "#%%\n",
    "\n",
    "#Random Forest Classifier\n",
    "\n",
    "def RandomForestC(data):\n",
    "    for k in kInstruments:\n",
    "        A = data[k][:]\n",
    "\n",
    "        startTime = datetime.now() \n",
    "\n",
    "        #quantitative testing with a RF classifier, score is summaried over N epoch\n",
    "        _Vscore = []\n",
    "        epoch = 10                                        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(A, y[k],\n",
    "                                                           train_size=0.8,\n",
    "                                                           random_state=None)\n",
    "\n",
    "        for i in range(epoch):\n",
    "\n",
    "            #    X_train2, X_test2, y_train2, y_test2 = train_test_split(X_train, y_train,\n",
    "            #                                                    train_size=0.8,\n",
    "            #                                                    random_state=None)\n",
    "            forest = RandomForestClassifier(n_estimators=2000, warm_start=False,\n",
    "                                               oob_score=False, max_features=0.3, random_state=None)\n",
    "            kfold = KFold(n_splits=5)\n",
    "\n",
    "            for train, test in kfold.split(X_train):\n",
    "\n",
    "                forest.fit(X_train[train], y_train[train])\n",
    "                _Vscore.append(forest.score(X_train[test], y_train[test]))   \n",
    "        print '\\n %s - RF Validation R2 score mean: %f \\n' % (k, np.mean(_Vscore))      \n",
    "        print '\\n %s - RF Validation R2 score std: %f \\n' % (k, np.std(_Vscore))\n",
    "        print '\\n %s - RF Test R2 score: %f \\n' % (k, forest.score(X_test, y_test))    \n",
    "        print '\\nExecuted in: %s. \\n ' % (str(datetime.now() - startTime))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SVC\n",
    "def SupportVectorC(data):\n",
    "    for k in kInstruments:\n",
    "        A = data[k][:]\n",
    "\n",
    "        startTime = datetime.now() \n",
    "\n",
    "        #quantitative testing with a RF classifier, score is summaried over N epoch\n",
    "        _Vscore = []\n",
    "        epoch = 100                                       \n",
    "        X_train, X_test, y_train, y_test = train_test_split(A, y[k],\n",
    "                                                           train_size=0.8,\n",
    "                                                           random_state=None)\n",
    "\n",
    "        for i in range(epoch):\n",
    "\n",
    "            svc = SVC(kernel='rbf', C=1, gamma='auto')\n",
    "            kfold = KFold(n_splits=5)\n",
    "\n",
    "            for train, test in kfold.split(X_train):\n",
    "\n",
    "                svc.fit(X_train[train], y_train[train])\n",
    "                _Vscore.append(svc.score(X_train[test], y_train[test]))\n",
    "        print '\\n %s - SVC Validation R2 score mean: %f \\n' % (k, np.mean(_Vscore))      \n",
    "        print '\\n %s - SVC Validation R2 score std: %f \\n' % (k, np.std(_Vscore))\n",
    "        print '\\n %s - SVC Test R2 score: %f \\n' % (k, svc.score(X_test, y_test))  \n",
    "        \n",
    "        \n",
    "        print '\\nExecuted in: %s. \\n ' % (str(datetime.now() - startTime))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LGC\n",
    "def LogisticRegressionC(data):\n",
    "    for k in kInstruments:\n",
    "        A = data[k][:]\n",
    "        startTime = datetime.now() \n",
    "\n",
    "        #quantitative testing with a RF classifier, score is summaried over N epoch\n",
    "        _Vscore = []\n",
    "        epoch = 100                                       \n",
    "        X_train, X_test, y_train, y_test = train_test_split(A, y[k],\n",
    "                                                           train_size=0.8,\n",
    "                                                           random_state=None)\n",
    "\n",
    "        for i in range(epoch):\n",
    "\n",
    "            clf = LogisticRegression(C=1,  tol=0.01)\n",
    "            \n",
    "            kfold = KFold(n_splits=5)\n",
    "\n",
    "            for train, test in kfold.split(X_train):\n",
    "\n",
    "                clf.fit(X_train[train], y_train[train])\n",
    "                _Vscore.append(clf.score(X_train[test], y_train[test]))\n",
    "\n",
    "        print '\\n %s - LG Validation R2 score mean: %f \\n' % (k, np.mean(_Vscore))      \n",
    "        print '\\n %s - LG Validation R2 score std: %f \\n' % (k, np.std(_Vscore))\n",
    "        print '\\n %s - LG Test R2 score: %f \\n' % (k, clf.score(X_test, y_test))    \n",
    "        print '\\nExecuted in: %s. \\n ' % (str(datetime.now() - startTime))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print '\\n Prediction RFC: \\n'\n",
    "RandomForestC(XPrediction)\n",
    "print '\\n Interpretation RFC: \\n'\n",
    "RandomForestC(XInterpretation)\n",
    "print '\\n Preselected RFC: \\n'\n",
    "RandomForestC(XPreSelected)\n",
    "print '\\n All RFC: \\n'\n",
    "RandomForestC(XAll)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print '\\n Prediction SVC: \\n'\n",
    "SupportVectorC(XPrediction)\n",
    "print '\\n Interpretation SVC: \\n'\n",
    "SupportVectorC(XInterpretation)\n",
    "print '\\n Preselected SVC: \\n'\n",
    "SupportVectorC(XPreSelected)\n",
    "print '\\n All SVC: \\n'\n",
    "SupportVectorC(XAll)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print '\\n Prediction LG: \\n'\n",
    "LogisticRegressionC(XPrediction)\n",
    "print '\\n Interpretation LG: \\n'\n",
    "LogisticRegressionC(XInterpretation)\n",
    "print '\\n Preselected LG: \\n'\n",
    "LogisticRegressionC(XPreSelected)\n",
    "print '\\n All LG: \\n'\n",
    "LogisticRegressionC(XAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quantitative analysis:   \n",
    "#%%\n",
    "\n",
    "#Random Forest Classifier\n",
    "\n",
    "def Classifiers(A_train, A_test, b_train, b_test):\n",
    "    #for k in kInstruments:\n",
    "        #A = data[k][:]\n",
    "\n",
    "        startTime = datetime.now() \n",
    "\n",
    "        #quantitative testing with a RF classifier, score is summaried over N epoch\n",
    "        _VRF = []\n",
    "        _VSV = []\n",
    "        _VLG = []\n",
    "        epoch = 100                                        \n",
    "        X_train = A_train[:]\n",
    "        X_test = A_test[:]\n",
    "        y_train = b_train[:]\n",
    "        y_test = b_test[:]\n",
    "        \n",
    "        \n",
    "        forest = RandomForestClassifier(n_estimators=500, warm_start=False,\n",
    "                                               oob_score=False, max_features='sqrt', random_state=None)\n",
    "            \n",
    "        clf = LogisticRegression(C=1,  tol=0.01)\n",
    "        svc = SVC(kernel='rbf', C=1, gamma='auto')\n",
    "        for i in range(epoch):\n",
    "\n",
    "            #    X_train2, X_test2, y_train2, y_test2 = train_test_split(X_train, y_train,\n",
    "            #                                                    train_size=0.8,\n",
    "            #                                                    random_state=None)\n",
    "            \n",
    "            \n",
    "            kfold = KFold(n_splits=5)\n",
    "\n",
    "            for train, test in kfold.split(X_train):\n",
    "\n",
    "                forest.fit(X_train[train], y_train[train])\n",
    "                _VRF.append(forest.score(X_train[test], y_train[test]))  \n",
    "                \n",
    "                clf.fit(X_train[train], y_train[train])\n",
    "                _VLG.append(clf.score(X_train[test], y_train[test]))\n",
    "                \n",
    "                svc.fit(X_train[train], y_train[train])\n",
    "                _VSV.append(svc.score(X_train[test], y_train[test]))\n",
    "                \n",
    "        print '\\n %s - RF Validation mean accuracy: %f \\n' % (kInstruments[k], np.mean(_VRF))      \n",
    "        print '\\n %s - RF Validation std mean accuracy: %f \\n' % (kInstruments[k], np.std(_VRF))\n",
    "        print '\\n %s - RF Test mean accuracy: %f \\n' % (kInstruments[k], forest.score(X_test, y_test))    \n",
    "        \n",
    "        print '\\n %s - SVC Validation mean accuracy: %f \\n' % (kInstruments[k], np.mean(_VSV))      \n",
    "        print '\\n %s - SVC Validation std mean accuracy: %f \\n' % (kInstruments[k], np.std(_VSV))\n",
    "        print '\\n %s - SVC Test mean accuracy: %f \\n' % (kInstruments[k], svc.score(X_test, y_test))  \n",
    "        \n",
    "        print '\\n %s - LG Validation mean accuracy: %f \\n' % (kInstruments[k], np.mean(_VLG))      \n",
    "        print '\\n %s - LG Validation std mean accuracy: %f \\n' % (kInstruments[k], np.std(_VLG))\n",
    "        print '\\n %s - LG Test mean accuracy: %f \\n' % (kInstruments[k], clf.score(X_test, y_test))\n",
    "        \n",
    "        print '\\nExecuted in: %s. \\n ' % (str(datetime.now() - startTime)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "\n",
    "XP_train, XP_test, XI_train, XI_test, XAll_train, XAll_test, y_train, y_test = train_test_split(XPrediction[kInstruments[k]],\n",
    "                                                                                                                                      XInterpretation[kInstruments[k]],\n",
    "                                                                                                                                      \n",
    "                                                                                                                                      XAll[kInstruments[k]],                                                                                                                                      \n",
    "                                                                                                                                      y[kInstruments[k]],\n",
    "                                                                                                                                      train_size=0.8,\n",
    "                                                                                                                                      random_state=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print '\\n %s \\n'% kInstruments[k]\n",
    "print '\\n Prediction Features: \\n'\n",
    "Classifiers(XP_train, XP_test, y_train, y_test)\n",
    "print '\\n Interpretation Features: \\n'\n",
    "Classifiers(XI_train, XI_test, y_train, y_test)\n",
    "\n",
    "print '\\n All Features: \\n'\n",
    "Classifiers(XAll_train, XAll_test,y_train, y_test)                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=1\n",
    "\n",
    "XP_train, XP_test, XI_train, XI_test, XAll_train, XAll_test, y_train, y_test = train_test_split(XPrediction[kInstruments[k]],\n",
    "                                                                                                                                      XInterpretation[kInstruments[k]],\n",
    "                                                                                                                                      \n",
    "                                                                                                                                      XAll[kInstruments[k]],                                                                                                                                      \n",
    "                                                                                                                                      y[kInstruments[k]],\n",
    "                                                                                                                                      train_size=0.8,\n",
    "                                                                                                                                      random_state=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print '\\n %s \\n'% kInstruments[k]\n",
    "print '\\n Prediction Features: \\n'\n",
    "Classifiers(XP_train, XP_test, y_train, y_test)\n",
    "print '\\n Interpretation Features: \\n'\n",
    "Classifiers(XI_train, XI_test, y_train, y_test)\n",
    "\n",
    "print '\\n All Features: \\n'\n",
    "Classifiers(XAll_train, XAll_test,y_train, y_test)                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=2\n",
    "\n",
    "XP_train, XP_test, XI_train, XI_test, XAll_train, XAll_test, y_train, y_test = train_test_split(XPrediction[kInstruments[k]],\n",
    "                                                                                                                                      XInterpretation[kInstruments[k]],\n",
    "                                                                                                                                      \n",
    "                                                                                                                                      XAll[kInstruments[k]],                                                                                                                                      \n",
    "                                                                                                                                      y[kInstruments[k]],\n",
    "                                                                                                                                      train_size=0.8,\n",
    "                                                                                                                                      random_state=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print '\\n %s \\n'% kInstruments[k]\n",
    "print '\\n Prediction Features: \\n'\n",
    "Classifiers(XP_train, XP_test, y_train, y_test)\n",
    "print '\\n Interpretation Features: \\n'\n",
    "Classifiers(XI_train, XI_test, y_train, y_test)\n",
    "\n",
    "print '\\n All Features: \\n'\n",
    "Classifiers(XAll_train, XAll_test,y_train, y_test)                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=3\n",
    "\n",
    "XP_train, XP_test, XI_train, XI_test, XAll_train, XAll_test, y_train, y_test = train_test_split(XPrediction[kInstruments[k]],\n",
    "                                                                                                                                      XInterpretation[kInstruments[k]],\n",
    "                                                                                                                                      \n",
    "                                                                                                                                      XAll[kInstruments[k]],                                                                                                                                      \n",
    "                                                                                                                                      y[kInstruments[k]],\n",
    "                                                                                                                                      train_size=0.8,\n",
    "                                                                                                                                      random_state=None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print '\\n %s \\n'% kInstruments[k]\n",
    "print '\\n Prediction Features: \\n'\n",
    "Classifiers(XP_train, XP_test, y_train, y_test)\n",
    "print '\\n Interpretation Features: \\n'\n",
    "Classifiers(XI_train, XI_test, y_train, y_test)\n",
    "\n",
    "print '\\n All Features: \\n'\n",
    "Classifiers(XAll_train, XAll_test,y_train, y_test)                                                                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " keys-all3 \n",
    "\n",
    "\n",
    " Prediction Features: \n",
    "\n",
    "\n",
    " keys-all3 - RF Validation mean accuracy: 0.638657 \n",
    "\n",
    "\n",
    " keys-all3 - RF Validation std mean accuracy: 0.054666 \n",
    "\n",
    "\n",
    " keys-all3 - RF Test mean accuracy: 0.692308 \n",
    "\n",
    "\n",
    " keys-all3 - SVC Validation mean accuracy: 0.711429 \n",
    "\n",
    "\n",
    " keys-all3 - SVC Validation std mean accuracy: 0.067586 \n",
    "\n",
    "\n",
    " keys-all3 - SVC Test mean accuracy: 0.653846 \n",
    "\n",
    "\n",
    " keys-all3 - LG Validation mean accuracy: 0.680952 \n",
    "\n",
    "\n",
    " keys-all3 - LG Validation std mean accuracy: 0.106053 \n",
    "\n",
    "\n",
    " keys-all3 - LG Test mean accuracy: 0.538462 \n",
    "\n",
    "\n",
    "Executed in: 0:11:45.990101. \n",
    " \n",
    "\n",
    " Interpretation Features: \n",
    "\n",
    "\n",
    " keys-all3 - RF Validation mean accuracy: 0.676381 \n",
    "\n",
    "\n",
    " keys-all3 - RF Validation std mean accuracy: 0.099669 \n",
    "\n",
    "\n",
    " keys-all3 - RF Test mean accuracy: 0.730769 \n",
    "\n",
    "\n",
    " keys-all3 - SVC Validation mean accuracy: 0.700952 \n",
    "\n",
    "\n",
    " keys-all3 - SVC Validation std mean accuracy: 0.091270 \n",
    "\n",
    "\n",
    " keys-all3 - SVC Test mean accuracy: 0.653846 \n",
    "\n",
    "\n",
    " keys-all3 - LG Validation mean accuracy: 0.739524 \n",
    "\n",
    "\n",
    " keys-all3 - LG Validation std mean accuracy: 0.096228 \n",
    "\n",
    "\n",
    " keys-all3 - LG Test mean accuracy: 0.653846 \n",
    "\n",
    "\n",
    "Executed in: 0:11:49.666839. \n",
    " \n",
    "\n",
    " All Features: \n",
    "\n",
    "\n",
    " keys-all3 - RF Validation mean accuracy: 0.644919 \n",
    "\n",
    "\n",
    " keys-all3 - RF Validation std mean accuracy: 0.060663 \n",
    "\n",
    "\n",
    " keys-all3 - RF Test mean accuracy: 0.653846 \n",
    "\n",
    "\n",
    " keys-all3 - SVC Validation mean accuracy: 0.431905 \n",
    "\n",
    "\n",
    " keys-all3 - SVC Validation std mean accuracy: 0.062807 \n",
    "\n",
    "\n",
    " keys-all3 - SVC Test mean accuracy: 0.461538 \n",
    "\n",
    "\n",
    " keys-all3 - LG Validation mean accuracy: 0.441905 \n",
    "\n",
    "\n",
    " keys-all3 - LG Validation std mean accuracy: 0.052025 \n",
    "\n",
    "\n",
    " keys-all3 - LG Test mean accuracy: 0.461538 \n",
    "\n",
    "\n",
    "Executed in: 0:14:02.908636. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(0.607-0.714)+(0.606-0.788)+(0.666-0.833)+(0.653-0.692)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.495/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
